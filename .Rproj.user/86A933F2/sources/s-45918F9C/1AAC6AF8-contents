
library(Rcpp)
library(dplyr)
library(hybridml)
library(BDgraph)
library(graphml)




Rcpp::sourceCpp("C:/Users/ericc/graphml/src/gwish.cpp")



# ------------------------------------------------------------------------------

library(dplyr)
library(BDgraph)
library(hybridml)
source("C:/Users/ericc/Documents/hybridml/examples/gwish/gwish_density.R")
source("C:/Users/ericc/Documents/hybridml/R/gwish_calc.R")

init_graph = function(G, b, n, V) {

  # input:
  # G: graph in adjacency matrix form
  # b: degrees of freedom for g-wishart distribution
  # n: pseudo-sample size, used to initialize the scale matrix
  # V: unscaled matrix
  #
  # output: list with various quantities that will be used in future calcs
  # G:         graph
  # b:         degrees of freedom
  # n:         pseudo-sample size, used to initialize the scale matrix
  # p:         dimension of graph
  # V_n:       unscaled matrix
  # S:         S = X'X when there is data (for posteriors)
  # P:         upper cholesky factor of the scale matrix V_n
  # D:         dimension of parameter space (# of free parameters)
  # edgeInd:   bool indicator for edges in G (upper tri + diag elements only)
  # nu_i       defined in step 2, p. 329 of Atay
  # b_i:       defined in step 2, p. 329 of Atay
  # t_ind:     (row,col) of the location of each of the free parameters
  # n_nonfree: number of nonfree parameters
  # vbar:      nonfree elements

  p   = ncol(G)           # dimension fo the graph
  V_n = n * V             # scale matrix for gW distribution
  P   = chol(solve(V_n))  # upper cholesky factor; D^(-1) = TT' in Atay paper
  S   = matrix(0, p, p)   # S = X'X when there is data (for posteriors)

  FREE_PARAMS_ALL = c(upper.tri(diag(1, p), diag = T) & G)
  edgeInd = G[upper.tri(G, diag = TRUE)] %>% as.logical

  ## construct A matrix so that we can compute k_i
  A = (upper.tri(diag(1, p), diag = F) & G) + 0

  k_i  = colSums(A) # see step 2, p. 329 of Atay
  nu_i = rowSums(A) # see step 2, p. 329 of Atay
  b_i = nu_i + k_i + 1

  D = sum(edgeInd) # number of free parameters / dimension of parameter space

  index_mat = matrix(0, p, p)
  index_mat[upper.tri(index_mat, diag = T)][edgeInd] = 1:D
  index_mat[upper.tri(index_mat, diag = T)]
  t_ind = which(index_mat!=0,arr.ind = T)

  index_mat[lower.tri(index_mat)] = NA
  vbar = which(index_mat==0,arr.ind = T) # non-free elements
  n_nonfree = nrow(vbar)



  obj = list(G = G, b = b, n = n, p = p, V_n = V_n, S = S, P = P, P_inv = solve(P),
             D = D, edgeInd = edgeInd, FREE_PARAMS_ALL = FREE_PARAMS_ALL,
             nu_i = nu_i, b_i = b_i,
             t_ind = t_ind, n_nonfree = n_nonfree, vbar = vbar,
             n_graphs = 1)

  return(obj)
}
gwish_globalMode_mod = function(u_df, params, params_G5,
                                tolerance = 1e-5, maxsteps = 200) {


  # use the MAP as the starting point for the algorithm
  MAP_LOC = which(u_df$psi_u == min(u_df$psi_u))
  theta = u_df[MAP_LOC,1:params$D] %>% unname() %>% unlist()

  numsteps = 0
  tolcriterion = 100
  step.size = 1


  while(tolcriterion > tolerance && numsteps < maxsteps){
    # print(numsteps)
    # hess_obj = hess(theta, params_G5)
    G = -hess(theta, params_G5)
    invG = solve(G)
    # G = -hess(theta, params)
    # invG = solve(G)

    thetaNew = theta + step.size * invG %*% grad(theta, params_G5)

    # if precision turns negative or if the posterior probability of
    # thetaNew becomes smaller than the posterior probability of theta
    if(-psi(thetaNew, params) < -psi(theta, params)) {
      cat('tolerance reached on log scale =', tolcriterion, '\n')
      print(paste("converged -- ", numsteps, " iters", sep = ''))
      return(theta)
    }

    tolcriterion = abs(psi(thetaNew, params)-psi(theta, params))
    theta = thetaNew
    numsteps = numsteps + 1
  }

  if(numsteps == maxsteps)
    warning('Maximum number of steps reached in Newton method.')

  print(paste("converged -- ", numsteps, " iters", sep = ''))
  return(theta)
}
hybml_gwish_cpp = function(u_df, params, psi, grad, hess, u_0 = NULL, D = ncol(u_df) - 1) {

  options(scipen = 999)
  options(dplyr.summarise.inform = FALSE)

  ## fit the regression tree via rpart()
  u_rpart = rpart::rpart(psi_u ~ ., u_df)

  ## (3) process the fitted tree
  # (3.1) obtain the (data-defined) support for each of the parameters
  param_support = extractSupport(u_df, D) #

  # (3.2) obtain the partition
  u_partition = extractPartition(u_rpart, param_support)

  #### hybrid extension begins here ------------------------------------------

  ### (1) find global mean
  # u_0 = colMeans(u_df[,1:D]) %>% unname() %>% unlist() # global mean

  if (is.null(u_0)) {
    MAP_LOC = which(u_df$psi_u == min(u_df$psi_u))
    u_0 = u_df[MAP_LOC,1:D] %>% unname() %>% unlist()
    # print(u_0)
  }

  ### (2) find point in each partition closest to global mean (for now)
  # u_k for each partition
  u_df_part = u_df %>% dplyr::mutate(leaf_id = u_rpart$where)

  l1_cost = apply(u_df_part[,1:D], 1, l1_norm, u_0 = u_0)
  u_df_part = u_df_part %>% dplyr::mutate(l1_cost = l1_cost)

  # take min result, group_by() leaf_id
  psi_df = u_df_part %>%
    dplyr::group_by(leaf_id) %>% dplyr::filter(l1_cost == min(l1_cost)) %>%
    data.frame

  bounds = u_partition %>% dplyr::arrange(leaf_id) %>%
    dplyr::select(-c("psi_hat", "leaf_id"))
  psi_df = psi_df %>% dplyr::arrange(leaf_id)

  K = nrow(bounds)
  log_terms = numeric(K) # store terms so that we can use log-sum-exp()
  G_k = numeric(K)       # store terms coming from gaussian integral

  # lambda_k = apply(psi_df[,1:D], 1, lambda, params = params)
  # k = 1
  for (k in 1:K) {
    u_k = unname(unlist(psi_df[k,1:D]))

    # hess_obj = hess(u_k, params) # pass in the G5 params, NOT the G params
    H_k = hess(u_k, params)
    # H_k_inv = solve(H_k)

    # H_k = hess(u_k, params = params)
    H_k_inv = suppressWarnings(chol2inv(chol(H_k)))

    # lambda_k = pracma::grad(psi, u_k, params = params) # numerical
    lambda_k = grad(u_k, params)
    b_k = H_k %*% u_k - lambda_k
    m_k = H_k_inv %*% b_k

    lb = bounds[k, seq(1, 2 * D, 2)] %>% unname %>% unlist
    ub = bounds[k, seq(2, 2 * D, 2)] %>% unname %>% unlist

    G_k[k] = epmgp::pmvn(lb, ub, m_k, H_k_inv, log = TRUE)
    # G_k[k] = log(TruncatedNormal::pmvnorm(m_k, H_k_inv, lb, ub)[1])

    log_terms[k] = D / 2 * log(2 * pi) - 0.5 * log_det(H_k) -
      psi_df$psi_u[k] + sum(lambda_k * u_k) -
      0.5 * t(u_k) %*% H_k %*% u_k +
      0.5 * t(m_k) %*% H_k %*% m_k + G_k[k]

    print(0.5 * t(u_k) %*% H_k %*% u_k)
  }

  return(list(logz = log_sum_exp(log_terms),
              bounds = bounds,
              G_k = G_k))
}


h = function() {
  options(scipen = 999)
  options(dplyr.summarise.inform = FALSE)

  ## fit the regression tree via rpart()
  u_rpart = rpart::rpart(psi_u ~ ., u_df)

  ## (3) process the fitted tree
  # (3.1) obtain the (data-defined) support for each of the parameters
  param_support = extractSupport(u_df, G5_obj$D) #

  # (3.2) obtain the partition
  u_partition = extractPartition(u_rpart, param_support)

  #### hybrid extension begins here ------------------------------------------

  ### (1) find global mean
  # u_0 = colMeans(u_df[,1:D]) %>% unname() %>% unlist() # global mean

  if (is.null(u_star)) {
    MAP_LOC = which(u_df$psi_u == min(u_df$psi_u))
    u_star = u_df[MAP_LOC,1:D] %>% unname() %>% unlist()
    # print(u_0)
  }

  ### (2) find point in each partition closest to global mean (for now)
  # u_k for each partition
  u_df_part = u_df %>% dplyr::mutate(leaf_id = u_rpart$where)

  l1_cost = apply(u_df_part[,1:G5_obj$D], 1, l1_norm, u_0 = u_star)
  u_df_part = u_df_part %>% dplyr::mutate(l1_cost = l1_cost)

  # take min result, group_by() leaf_id
  psi_df = u_df_part %>%
    dplyr::group_by(leaf_id) %>% dplyr::filter(l1_cost == min(l1_cost)) %>%
    data.frame

  bounds = u_partition %>% dplyr::arrange(leaf_id) %>%
    dplyr::select(-c("psi_hat", "leaf_id"))
  psi_df = psi_df %>% dplyr::arrange(leaf_id)

  K = nrow(bounds)
  approx_integral(K, as.matrix(psi_df), as.matrix(bounds), G5_obj)
}



G_5 = matrix(c(1,1,0,1,1,
               1,1,1,0,0,
               0,1,1,1,1,
               1,0,1,1,1,
               1,0,1,1,1), 5, 5)

G = G_5
n = 10
V = diag(1, ncol(G_5))
b = 5
J = 200
p = ncol(G_5)

G5_obj = init_graph(G = G_5, b = 5, n = n, V = diag(1, ncol(G_5)))
grad = function(u, params) { fast_grad(u, params) }
hess = function(u, params) { fast_hess(u, params) }


J = 2000
samps = samplegw(J, G5_obj$G, G5_obj$b, 0, G5_obj$V_n, G5_obj$S, solve(G5_obj$P),
                 G5_obj$FREE_PARAMS_ALL)

u_samps = samps$Psi_free %>% data.frame

## gwish_preprocess() function is in gwish_calc.R file in hybridml package
u_df = gwish_preprocess(u_samps,G5_obj$D, G5_obj)     # J x (D_u + 1)

## gwish_globalMode() function is in gwish_calc.R file in hybridml package
u_star = gwish_globalMode(u_df, G5_obj, G5_obj)

## gwish_globalMode_mod() function is in THIS file and uses the cpp functions

logzhat = hybml_gwish(u_df, G5_obj, psi = psi, grad = grad, hess = hess,
                      u_0 = u_star)$logz
logzhat

### compute using the graphml package
h()

microbenchmark::microbenchmark(r = hybml_gwish(u_df, G5_obj, psi = psi, grad = grad, hess = hess,
                                               u_0 = u_star)$logz,
                               cpp = h())


set.seed(2021)

p = 40; n = 300

G = matrix(0, p, p)

G[1, p] = 1

for(i in 1:(p-1)){

  G[i, i+1] = 1

}

G= G + t(G); diag(G) = 1

Sigma = rgwish(1, G, 3, diag(p))

Y = matrix(rnorm(n*p,), n, p)

Y = Y %*% t(chol(Sigma))

gnorm  (G, n+3, diag(p)+t(Y)%*%Y, 500)

G5_obj = init_graph(G = G, b = 3, n = n, V = Sigma)












